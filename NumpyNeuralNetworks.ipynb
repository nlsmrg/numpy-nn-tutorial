{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c31485-068d-42da-a581-a35193e6fcf9",
   "metadata": {},
   "source": [
    "# Relevant Literature\n",
    "\n",
    "1. **Dive into Deep Learning**  \n",
    "   A mix of a book and interactive tutorial that covers deep learning fundamentals with hands‑on examples.  \n",
    "   Available at: [https://d2l.ai](https://d2l.ai)\n",
    "\n",
    "2. **PyTorch Blitz Tutorial**  \n",
    "   A beginner‑friendly tutorial that provides a quick yet thorough introduction to building and training deep learning models using PyTorch.  \n",
    "   Available at: [https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "\n",
    "3. **Deep Learning: An Introduction for Applied Mathematicians**  \n",
    "   A mathematically oriented paper that explains the basics of deep learning in a way that is accessible for mathematicians, physicists, and engineers.  \n",
    "   Available at: [https://epubs.siam.org/doi/10.1137/18M1165748](https://epubs.siam.org/doi/10.1137/18M1165748)\n",
    "\n",
    "4. **Deep Learning**  \n",
    "   *Ian Goodfellow, Yoshua Bengio, and Aaron Courville*  \n",
    "   A comprehensive textbook that has become a standard reference in the field, offering a solid introduction along with theoretical details and practical applications.  \n",
    "   Available at: [https://www.deeplearningbook.org](https://www.deeplearningbook.org)\n",
    "\n",
    "5. **Neural Networks and Deep Learning**  \n",
    "   *Charu C. Aggarwal*  \n",
    "   A clear and mathematically rigorous introduction to neural networks and deep learning, suitable for those seeking a deeper theoretical foundation.  \n",
    "   Available at: [https://www.springer.com/gp/book/9783319944623](https://www.springer.com/gp/book/9783319944623)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8843c2a4-426f-4035-8da7-81c2efeabef8",
   "metadata": {},
   "source": [
    "# Machine Learning and Artificial Neural Networks\n",
    "\n",
    "Machine Learning is a discipline concerned with the automatic recognition of patterns and regularities in data, and with using these patterns to predict unknown data or make decisions. It provides tools that allow computers to learn from examples rather than following explicitly programmed instructions. This field has revolutionized many areas, from image and speech recognition to medical diagnosis and financial forecasting.\n",
    "\n",
    "In machine learning, we often distinguish between two major types of learning:\n",
    "\n",
    "- **Unsupervised Learning:**  \n",
    "  Given input data \n",
    "  $$\n",
    "  \\{ \\mathbf{x}_i \\}_{i=1}^N \\subset \\mathcal{X},\n",
    "  $$\n",
    "  the goal is to discover inherent patterns, structures, or groupings in the data without any predefined labels or targets. Common tasks include clustering (grouping similar items), dimensionality reduction (simplifying data while retaining its structure), and density estimation. This task is challenging because there is no straightforward error metric to judge which patterns are “correct” — instead, the quality of the discovered patterns often depends on the context and the specific application.\n",
    "\n",
    "- **Supervised Learning:**  \n",
    "  Given input data \n",
    "  $$\n",
    "  \\{ \\mathbf{x}_i \\}_{i=1}^N \\subset \\mathcal{X} \\quad \\text{and} \\quad \\{ \\mathbf{y}_i \\}_{i=1}^N \\subset \\mathcal{Y},\n",
    "  $$\n",
    "  the goal is to learn a function\n",
    "  $$\n",
    "  \\hat{f} : \\mathcal{X} \\to \\mathcal{Y}\n",
    "  $$\n",
    "  that maps inputs to outputs. In classification tasks, outputs are discrete (e.g., $\\mathbf{y}_i \\in \\{1,\\dots,C\\}$), while in regression tasks, they are continuous (e.g., $\\mathbf{y}_i \\in \\mathbb{R}^n$). The performance of the learned function is usually measured by a loss or error metric that quantifies how far the predictions are from the true labels. This approach has been applied successfully in many fields such as natural language processing, computer vision, and bioinformatics.\n",
    "\n",
    "## Artificial Neural Networks\n",
    "\n",
    "Artificial Neural Networks (ANNs) are a specific class of models used in supervised learning (and sometimes unsupervised learning) that are loosely inspired by the biological networks in human brains. The key ideas behind ANNs are:\n",
    "\n",
    "- **Layers of Neurons:**  \n",
    "  An ANN is composed of multiple layers. Each layer consists of many small computational units called neurons. Neurons in one layer are connected to neurons in the subsequent layer, and each connection has an associated weight that represents its importance.\n",
    "\n",
    "- **Forward Propagation:**  \n",
    "  When an input is fed into the network, it is passed through these layers. Each neuron applies a linear transformation to its inputs (using the connection weights) and then passes the result through a non-linear activation function. This process is known as forward propagation, and it allows the network to build increasingly complex representations of the input data.\n",
    "\n",
    "- **Backpropagation and Learning:**  \n",
    "  To train an ANN, we adjust the weights so that the network’s predictions become closer to the actual targets. This is done by computing the error (or loss) between the predictions and the true labels and then propagating this error back through the network. The backpropagation algorithm calculates gradients for each weight, which are then used by an optimizer to update the weights. This iterative process enables the network to learn from the data.\n",
    "\n",
    "- **Deep Networks:**  \n",
    "  Modern ANNs often have many layers (hence \"deep\" learning), which allows them to learn hierarchical features from raw data. For instance, in image recognition, early layers might detect edges, while deeper layers recognize shapes, textures, or even entire objects.\n",
    "\n",
    "While there are other settings (e.g., reinforcement learning), in this notebook we focus on the supervised case, which is most common in applications of artificial neural networks.\n",
    "\n",
    "This introduction motivates a more mathematical description of neural networks and their implementation. This includes the construction of layers, the definition of cost functions, and the development of optimizers. In the subsequent sections, we will see how these components come together to form a complete neural network model capable of learning patterns from data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba8a040-bf6d-41fd-8fa1-ad05df13db66",
   "metadata": {},
   "source": [
    "## Function Approximation and Optimization in Machine Learning\n",
    "\n",
    "In machine learning, our primary goal is to learn a function that can predict or approximate unknown relationships in data. We start with the assumption that there exists an unknown function\n",
    "$$\n",
    "f: \\mathcal{X} \\to \\mathcal{Y},\n",
    "$$\n",
    "which exactly maps each input to its corresponding output on the training data:\n",
    "$$\n",
    "f(\\mathbf{x}_i) = \\mathbf{y}_i \\quad \\text{for } i = 1, \\dots, N.\n",
    "$$\n",
    "\n",
    "Because we do not know the true function $f$, we aim to find a predictor $\\hat{f}$ from a set of candidate functions $\\mathcal{F}$ that approximates $f$ as well as possible using the training set\n",
    "$$\n",
    "\\{ (\\mathbf{x}_i, \\mathbf{y}_i) \\}_{i=1}^N.\n",
    "$$\n",
    "\n",
    "This goal can be formalized as the following optimization problem:\n",
    "$$\n",
    "\\min_{\\hat{f} \\in \\mathcal{F}} \\frac{1}{N} \\sum_{i=1}^{N} J\\bigl(\\hat{f}(\\mathbf{x}_i),\\,\\mathbf{y}_i\\bigr) + \\Omega(\\hat{f}),\n",
    "$$\n",
    "where:\n",
    "\n",
    "- **$J$:** The loss function $J: \\mathcal{Y} \\times \\mathcal{Y} \\to \\mathbb{R}$ measures the error between the network's prediction $\\hat{f}(\\mathbf{x}_i)$ and the true output $\\mathbf{y}_i$. For example, if we use the squared error loss, then $J$ might be defined as\n",
    "  $$\n",
    "  J\\bigl(\\hat{f}(\\mathbf{x}_i),\\,\\mathbf{y}_i\\bigr) = \\frac{1}{2} \\| \\hat{f}(\\mathbf{x}_i) - \\mathbf{y}_i \\|_2^2.\n",
    "  $$\n",
    "\n",
    "- **$\\Omega$:** The regularization function $\\Omega: \\mathcal{F} \\to \\mathbb{R}_{\\ge 0}$ is added to prevent the model from simply memorizing the training data—a situation known as overfitting. Regularization introduces a penalty for overly complex models, thus encouraging the predictor to generalize better to unseen data.\n",
    "\n",
    "- **$\\mathcal{F}$:** The space of candidate functions from which the predictor $\\hat{f}$ is chosen.\n",
    "\n",
    "The combination of the loss function and the regularization term forms our **objective function**. Minimizing this objective over $\\mathcal{F}$ is the goal of training a machine learning model.\n",
    "\n",
    "### Bias–Variance Decomposition\n",
    "\n",
    "For regression tasks using the squared error loss, it is helpful to understand how the prediction error can be broken down into three main components:\n",
    "$$\n",
    "\\mathbb{E}\\Bigl[(\\mathbf{y} - \\hat{f}(\\mathbf{x}))^2\\Bigr] = \\bigl(\\operatorname{Bias}\\,\\hat{f}(\\mathbf{x})\\bigr)^2 + \\operatorname{Var}\\,\\hat{f}(\\mathbf{x}) + \\sigma^2.\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- **Bias:** \n",
    "  $$\n",
    "  \\operatorname{Bias}\\,\\hat{f}(\\mathbf{x}) = \\mathbb{E}\\bigl[\\hat{f}(\\mathbf{x})\\bigr] - f(\\mathbf{x})\n",
    "  $$\n",
    "  measures the error introduced by approximating the true function $f$ with a simpler model. High bias often means that the model is too simple to capture the underlying pattern (underfitting).\n",
    "\n",
    "- **Variance:** \n",
    "  $$\n",
    "  \\operatorname{Var}\\,\\hat{f}(\\mathbf{x}) = \\mathbb{E}\\Bigl[\\hat{f}(\\mathbf{x})^2\\Bigr] - \\Bigl(\\mathbb{E}\\bigl[\\hat{f}(\\mathbf{x})\\bigr]\\Bigr)^2\n",
    "  $$\n",
    "  captures how much the predictions $\\hat{f}(\\mathbf{x})$ would vary if we trained the model on different data samples. High variance means the model is overly sensitive to fluctuations in the training data (overfitting).\n",
    "\n",
    "- **Noise ($\\sigma^2$):**  \n",
    "  This term represents the inherent randomness or noise in the data that cannot be predicted by any model.\n",
    "\n",
    "A central challenge in designing learning algorithms is to balance bias and variance. More complex models typically reduce bias but increase variance. Regularization helps by adding a small amount of bias to reduce the variance, leading to better overall generalization on new, unseen data.\n",
    "\n",
    "In summary, machine learning can be seen as the problem of function approximation, where we aim to find a function $\\hat{f}$ that not only fits the training data well (minimizing the loss) but also generalizes to new data (aided by regularization). The bias–variance trade-off clarifies the fundamental challenge of this task: We need to strike the right balance between model complexity and robustness.\n",
    "\n",
    "This formulation serves as the foundation for understanding neural networks, which have become one of the most powerful tools for function approximation. In our setting, we use multilayered networks composed of simple neurons, and we train them using optimization methods such as the stochastic gradient method. The following sections of this notebook will build on these ideas, providing hands-on examples and code to illustrate how these concepts come together in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9fa281-4c42-4ba6-a1df-ebc4baa244fa",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks (ANNs)\n",
    "\n",
    "From the abstract function approximation perspective, an artificial neural network (ANN) is a parameterized function built as the composition of several layers. Each layer is a nonlinear mapping:\n",
    "$$\n",
    "\\mathbf{f}_i : \\mathcal{E}_i \\times \\mathcal{H}_i \\to \\mathcal{E}_{i+1},\n",
    "$$\n",
    "where:\n",
    "- $\\mathcal{E}_i$ is the state space (the space of inputs at layer $i$),\n",
    "- $\\mathcal{H}_i$ is the parameter space for the $i$-th layer,\n",
    "- The full network is given by\n",
    "  $$\n",
    "  F(\\mathbf{x}; \\mathcal{W}) = \\bigl( f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1 \\bigr)(\\mathbf{x}; \\mathcal{W}),\n",
    "  $$\n",
    "  with $\\mathcal{W} = \\{ \\mathbf{W}_1, \\dots, \\mathbf{W}_L \\}$ representing the set of all trainable parameters.\n",
    "\n",
    "### Backpropagation and Gradient Descent\n",
    "\n",
    "We already know that we have to train or optimize the network with respect to a goal. Usually, the goal is the approximation of the exact (unknown) function $f$. We formalized this problem as a minimization problem, where we seek to minimize a cost (loss) function. For example, in a regression problem the loss can be defined as:\n",
    "$$\n",
    "J(\\mathbf{x}; \\mathcal{W}) = \\frac{1}{2}\\| F(\\mathbf{x}; \\mathcal{W}) - \\mathbf{y} \\|_2^2.\n",
    "$$\n",
    "\n",
    "Using the chain rule, the gradient of the network output with respect to the parameters of layer $i$ is given by:\n",
    "$$\n",
    "\\nabla_{\\mathbf{W}_i} F(\\mathbf{x}; \\mathcal{W}) = D\\mathbf{t}_{i+1}\\bigl(f_i(\\mathbf{x}^i)\\bigr) \\cdot \\nabla_{\\mathbf{W}_i} f_i(\\mathbf{x}^i),\n",
    "$$\n",
    "where $\\mathbf{x}^i$ is the input to the $i$-th layer, and $\\mathbf{t}_{i+1}$ represents the composition of the subsequent layers. The gradient of the cost function then is:\n",
    "$$\n",
    "\\nabla_{\\mathbf{W}_i} J(\\mathbf{x}; \\mathcal{W}) = \\nabla_{\\mathbf{W}_i}^* F(\\mathbf{x}; \\mathcal{W}) \\cdot \\bigl( F(\\mathbf{x}; \\mathcal{W}) - \\mathbf{y} \\bigr).\n",
    "$$\n",
    "\n",
    "This derivation underpins the **backpropagation algorithm**, which computes the gradients by propagating errors backward through the network.\n",
    "\n",
    "### Gradient Descent Algorithm\n",
    "\n",
    "A typical gradient descent algorithm for ANNs is summarized in the following pseudocode:\n",
    "\n",
    "1. **Forward Pass:**  \n",
    "   Compute the outputs $ \\mathbf{x}^1 = \\mathbf{x} $ and\n",
    "   $$\n",
    "   \\mathbf{x}^{i+1} = f_i(\\mathbf{x}^i) \\quad \\text{for } i = 1, \\dots, L.\n",
    "   $$\n",
    "\n",
    "2. **Backward Pass:**  \n",
    "   For $ i = L, \\dots, 1 $, compute the error:\n",
    "   $$\n",
    "   \\mathbf{e}_i = \n",
    "   \\begin{cases}\n",
    "   \\mathbf{x}^{L+1} - \\mathbf{y}, & i = L,\\\\[1mm]\n",
    "   D^* f_{i+1}(\\mathbf{x}^{i+1}) \\cdot \\mathbf{e}_{i+1}, & i = L-1, \\dots, 1,\n",
    "   \\end{cases}\n",
    "   $$\n",
    "   and then update the gradient:\n",
    "   $$\n",
    "   \\nabla_{\\mathbf{W}_i} J(\\mathbf{x}; \\mathcal{W}) = \\nabla_{\\mathbf{W}_i}^* f_i(\\mathbf{x}^i) \\cdot \\mathbf{e}_i.\n",
    "   $$\n",
    "\n",
    "3. **Parameter Update:**  \n",
    "   Update the weights:\n",
    "   $$\n",
    "   \\mathbf{W}_i \\leftarrow \\mathbf{W}_i - \\eta \\nabla_{\\mathbf{W}_i} J(\\mathbf{x}; \\mathcal{W}),\n",
    "   $$\n",
    "   where $\\eta$ is the learning rate.\n",
    "\n",
    "This procedure is repeated (often over mini-batches, leading to stochastic gradient descent) until the loss is minimized.\n",
    "\n",
    "### Universal Approximation Theorem\n",
    "\n",
    "The universal approximation theorem guarantees that a neural network with one hidden layer (given a suitable activation function) can approximate any continuous function on a compact set to arbitrary accuracy. Although this theorem is not constructive (the network size may grow very large), it provides a theoretical foundation for using ANNs in function approximation.\n",
    "\n",
    "---\n",
    "\n",
    "## Multilayer Perceptrons (MLP) and Advanced Architectures\n",
    "\n",
    "A **Multilayer Perceptron (MLP)** is a feedforward neural network that consists of:\n",
    "- A weight matrix $\\mathbf{W}_i \\in \\mathbb{R}^{n_i \\times n_{i+1}}$ for each layer,\n",
    "- A nonlinear activation function $\\sigma: \\mathbb{R} \\to \\mathbb{R}$, applied elementwise,\n",
    "- The layer operations are given by:\n",
    "  $$\n",
    "  \\begin{aligned}\n",
    "  f_1(\\mathbf{x}) &= \\mathbf{x}, \\\\\n",
    "  f_i(\\mathbf{x}) &= \\sigma\\bigl( \\mathbf{W}_i f_{i-1}(\\mathbf{x}) + \\mathbf{b}_i \\bigr), \\quad i = 2, \\dots, L.\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "\n",
    "Additional techniques to improve training include:\n",
    "- **Batch Normalization:** Scales and shifts activations to improve convergence.\n",
    "- **Skip Connections:** Add the input of a layer to its output (if dimensions match) to help with training deeper networks.\n",
    "\n",
    "Other architectures like convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformers extend these ideas for specific applications (e.g., image processing, sequential data, etc.). In more specialized areas such as solving partial differential equations (PDEs) or model order reduction, variants like Fourier neural operators and DeepONets are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622615cd-5ef6-4b4f-b1ab-42698869547e",
   "metadata": {},
   "source": [
    "# Backpropagation for a Multilayer Perceptron\n",
    "\n",
    "Consider a multilayer perceptron with one hidden layer. The network performs the following computations:\n",
    "\n",
    "1. **Hidden Layer:**  \n",
    "   - **Linear Combination:**  \n",
    "     $$\n",
    "     z^{(1)} = W^{(1)} x + b^{(1)}\n",
    "     $$\n",
    "   - **Activation:**  \n",
    "     $$\n",
    "     a^{(1)} = \\sigma\\left(z^{(1)}\\right)\n",
    "     $$\n",
    "     where $\\sigma(\\cdot)$ is an activation function (e.g., sigmoid, tanh, ReLU).\n",
    "\n",
    "2. **Output Layer:**  \n",
    "   - **Linear Combination:**  \n",
    "     $$\n",
    "     z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}\n",
    "     $$\n",
    "   - **Activation (Output):**  \n",
    "     $$\n",
    "     a^{(2)} = g\\left(z^{(2)}\\right)\n",
    "     $$\n",
    "     where $g(\\cdot)$ is the output activation function (e.g., sigmoid for binary classification or identity for regression).\n",
    "\n",
    "3. **Loss Function:**  \n",
    "   Let $ L(a^{(2)}, y) $ be the loss function that measures the discrepancy between the network’s output $a^{(2)}$ and the true label $y$. For example, one might use the Mean Squared Error (MSE) for regression:\n",
    "   $$\n",
    "   L = \\frac{1}{2}\\left( a^{(2)} - y \\right)^2.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## Backward Pass: Computing Gradients\n",
    "\n",
    "The goal of backpropagation is to compute the gradients of the loss $L$ with respect to the network parameters $W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)}$. We use the chain rule to propagate errors backward through the network.\n",
    "\n",
    "### Step 1: Output Layer Gradients\n",
    "\n",
    "Define the **error** at the output layer as:\n",
    "$$\n",
    "\\delta^{(2)} = \\frac{\\partial L}{\\partial z^{(2)}}.\n",
    "$$\n",
    "Using the chain rule:\n",
    "$$\n",
    "\\delta^{(2)} = \\frac{\\partial L}{\\partial a^{(2)}} \\circ g'\\left(z^{(2)}\\right),\n",
    "$$\n",
    "where $\\circ$ denotes the elementwise (Hadamard) product, and $g'\\left(z^{(2)}\\right)$ is the derivative of the output activation function evaluated at $z^{(2)}$.\n",
    "\n",
    "The gradients with respect to the output layer weights and biases are then:\n",
    "- **Weights:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)} \\left(a^{(1)}\\right)^T\n",
    "  $$\n",
    "- **Biases:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial b^{(2)}} = \\delta^{(2)}\n",
    "  $$\n",
    "\n",
    "### Step 2: Hidden Layer Gradients\n",
    "\n",
    "To backpropagate the error to the hidden layer, first compute the error term for the hidden layer:\n",
    "$$\n",
    "\\delta^{(1)} = \\frac{\\partial L}{\\partial z^{(1)}}.\n",
    "$$\n",
    "Using the chain rule:\n",
    "$$\n",
    "\\delta^{(1)} = \\left(W^{(2)}\\right)^T \\delta^{(2)} \\circ \\sigma'\\left(z^{(1)}\\right),\n",
    "$$\n",
    "where $\\sigma'\\left(z^{(1)}\\right)$ is the derivative of the hidden activation function.\n",
    "\n",
    "The gradients with respect to the hidden layer weights and biases are:\n",
    "- **Weights:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)} \\, x^T\n",
    "  $$\n",
    "- **Biases:**\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial b^{(1)}} = \\delta^{(1)}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Backward Functions\n",
    "\n",
    "For a given input $x$, target $y$, and computed activations:\n",
    "1. **Forward Pass:**\n",
    "   - $ z^{(1)} = W^{(1)} x + b^{(1)}$\n",
    "   - $ a^{(1)} = f\\left(z^{(1)}\\right)$\n",
    "   - $ z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}$\n",
    "   - $ a^{(2)} = g\\left(z^{(2)}\\right)$\n",
    "   - Loss: $L(a^{(2)}, y)$\n",
    "\n",
    "2. **Backward Pass:**\n",
    "   - **Output Layer:**\n",
    "     $$\n",
    "     \\delta^{(2)} = \\frac{\\partial L}{\\partial a^{(2)}} \\circ g'\\left(z^{(2)}\\right)\n",
    "     $$\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)} \\left(a^{(1)}\\right)^T,\\quad \\frac{\\partial L}{\\partial b^{(2)}} = \\delta^{(2)}\n",
    "     $$\n",
    "   - **Hidden Layer:**\n",
    "     $$\n",
    "     \\delta^{(1)} = \\left(W^{(2)}\\right)^T \\delta^{(2)} \\circ f'\\left(z^{(1)}\\right)\n",
    "     $$\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)} \\, x^T,\\quad \\frac{\\partial L}{\\partial b^{(1)}} = \\delta^{(1)}\n",
    "     $$\n",
    "\n",
    "These formulas constitute the backward functions for updating the network parameters during training via gradient descent.\n",
    "This derivation provides the foundation for implementing backpropagation in any multilayer perceptron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c14c86-0b7c-436d-8474-e313f20e391d",
   "metadata": {},
   "source": [
    "# One Step of Gradient Descent for a Multilayer Perceptron\n",
    "\n",
    "In this section, we map out one complete step of gradient descent. This includes the forward pass, loss computation, backpropagation (gradient computation), and the parameter update.\n",
    "\n",
    "Let's assume a simple two-layer network (one hidden layer) with the following architecture:\n",
    "- **Input Layer:** $x$\n",
    "- **Hidden Layer:** Weights $W^{(1)}$, Biases $b^{(1)}$, Activation function $\\sigma(\\cdot)$\n",
    "- **Output Layer:** Weights $W^{(2)}$, Biases $b^{(2)}$, Activation function $g(\\cdot)$\n",
    "- **Loss Function:** $L(a^{(2)}, y)$ where $a^{(2)}$ is the network output and $y$ is the true label.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Forward Pass\n",
    "\n",
    "1. **Compute Hidden Layer Pre-activation:**\n",
    "   $$\n",
    "   z^{(1)} = W^{(1)} x + b^{(1)}\n",
    "   $$\n",
    "2. **Apply Hidden Layer Activation:**\n",
    "   $$\n",
    "   a^{(1)} = \\sigma\\left(z^{(1)}\\right)\n",
    "   $$\n",
    "3. **Compute Output Layer Pre-activation:**\n",
    "   $$\n",
    "   z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}\n",
    "   $$\n",
    "4. **Apply Output Layer Activation:**\n",
    "   $$\n",
    "   a^{(2)} = g\\left(z^{(2)}\\right)\n",
    "   $$\n",
    "\n",
    "## 2. Loss Computation\n",
    "\n",
    "Compute the loss using a suitable loss function. For example, if using Mean Squared Error (MSE):\n",
    "$$\n",
    "L = \\frac{1}{2}\\left( a^{(2)} - y \\right)^2.\n",
    "$$\n",
    "\n",
    "## 3. Backward Pass (Gradient Computation)\n",
    "\n",
    "### Output Layer Gradients\n",
    "\n",
    "- **Error at Output Layer:**\n",
    "  $$\n",
    "  \\delta^{(2)} = \\frac{\\partial L}{\\partial a^{(2)}} \\circ g'\\left(z^{(2)}\\right)\n",
    "  $$\n",
    "  where $\\circ$ denotes elementwise multiplication and $g'\\left(z^{(2)}\\right)$ is the derivative of the output activation function.\n",
    "\n",
    "- **Gradients for Output Layer Parameters:**\n",
    "  - For weights:\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)} \\left(a^{(1)}\\right)^T\n",
    "    $$\n",
    "  - For biases:\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial b^{(2)}} = \\delta^{(2)}\n",
    "    $$\n",
    "\n",
    "### Hidden Layer Gradients\n",
    "\n",
    "- **Propagate Error to Hidden Layer:**\n",
    "  $$\n",
    "  \\delta^{(1)} = \\left(W^{(2)}\\right)^T \\delta^{(2)} \\circ \\sigma'\\left(z^{(1)}\\right)\n",
    "  $$\n",
    "  where $\\sigma'\\left(z^{(1)}\\right)$ is the derivative of the hidden activation function.\n",
    "\n",
    "- **Gradients for Hidden Layer Parameters:**\n",
    "  - For weights:\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)} \\, x^T\n",
    "    $$\n",
    "  - For biases:\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial b^{(1)}} = \\delta^{(1)}\n",
    "    $$\n",
    "\n",
    "## 4. Parameter Update (Gradient Descent)\n",
    "\n",
    "Using the learning rate $\\alpha$, update each parameter by moving in the opposite direction of the gradient:\n",
    "\n",
    "- **Output Layer Updates:**\n",
    "  $$\n",
    "  W^{(2)}_{\\text{new}} = W^{(2)} - \\alpha \\frac{\\partial L}{\\partial W^{(2)}}\n",
    "  $$\n",
    "  $$\n",
    "  b^{(2)}_{\\text{new}} = b^{(2)} - \\alpha \\frac{\\partial L}{\\partial b^{(2)}}\n",
    "  $$\n",
    "\n",
    "- **Hidden Layer Updates:**\n",
    "  $$\n",
    "  W^{(1)}_{\\text{new}} = W^{(1)} - \\alpha \\frac{\\partial L}{\\partial W^{(1)}}\n",
    "  $$\n",
    "  $$\n",
    "  b^{(1)}_{\\text{new}} = b^{(1)} - \\alpha \\frac{\\partial L}{\\partial b^{(1)}}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of One Gradient Descent Step\n",
    "\n",
    "1. **Forward Pass:**\n",
    "   - Compute $ z^{(1)} = W^{(1)} x + b^{(1)} $\n",
    "   - Compute $ a^{(1)} = \\sigma\\left(z^{(1)}\\right) $\n",
    "   - Compute $ z^{(2)} = W^{(2)} a^{(1)} + b^{(2)} $\n",
    "   - Compute $ a^{(2)} = g\\left(z^{(2)}\\right) $\n",
    "\n",
    "2. **Loss Calculation:**\n",
    "   - Compute $ L = \\frac{1}{2}\\left( a^{(2)} - y \\right)^2 $ (or another suitable loss)\n",
    "\n",
    "3. **Backward Pass:**\n",
    "   - Compute output error: \n",
    "     $\n",
    "     \\delta^{(2)} = \\frac{\\partial L}{\\partial a^{(2)}} \\circ g'\\left(z^{(2)}\\right)\n",
    "     $\n",
    "   - Compute gradients: \n",
    "     $\n",
    "     \\frac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)} \\left(a^{(1)}\\right)^T, \\quad \\frac{\\partial L}{\\partial b^{(2)}} = \\delta^{(2)}\n",
    "     $\n",
    "   - Propagate error: \n",
    "     $\n",
    "     \\delta^{(1)} = \\left(W^{(2)}\\right)^T \\delta^{(2)} \\circ \\sigma'\\left(z^{(1)}\\right)\n",
    "     $\n",
    "   - Compute gradients:\n",
    "     $\n",
    "     \\frac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)} \\, x^T, \\quad \\frac{\\partial L}{\\partial b^{(1)}} = \\delta^{(1)}\n",
    "     $\n",
    "\n",
    "4. **Update Parameters:**\n",
    "   - $ W^{(2)} \\leftarrow W^{(2)} - \\alpha \\frac{\\partial L}{\\partial W^{(2)}} $\n",
    "   - $ b^{(2)} \\leftarrow b^{(2)} - \\alpha \\frac{\\partial L}{\\partial b^{(2)}} $\n",
    "   - $ W^{(1)} \\leftarrow W^{(1)} - \\alpha \\frac{\\partial L}{\\partial W^{(1)}} $\n",
    "   - $ b^{(1)} \\leftarrow b^{(1)} - \\alpha \\frac{\\partial L}{\\partial b^{(1)}} $\n",
    "\n",
    "This completes one step of gradient descent for a simple multilayer perceptron.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b69bc-8f47-42f9-ad29-d837a8383277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0) \n",
    "# Activation functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(output):\n",
    "    return output * (1 - output)\n",
    "\n",
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]], dtype=float)\n",
    "y_xor = np.array([[0],\n",
    "                  [1],\n",
    "                  [1],\n",
    "                  [0]], dtype=float)\n",
    "\n",
    "# Network architecture: 2 inputs -> 2 hidden neurons -> 1 output\n",
    "input_dim = 2\n",
    "hidden_dim = 2\n",
    "output_dim = 1\n",
    "\n",
    "# Weight initialization\n",
    "np.random.seed(0)\n",
    "W1 = np.random.randn(input_dim, hidden_dim) * 0.5\n",
    "b1 = np.random.randn(hidden_dim) * 0.5\n",
    "W2 = np.random.randn(hidden_dim, output_dim) * 0.5\n",
    "b2 = np.random.randn(output_dim) * 0.5\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Training loop for XOR\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    z1 = X_xor.dot(W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    y_pred = sigmoid(z2)\n",
    "    if epoch%1000 == 0:\n",
    "        print(\"XOR Predictions in epoch \", epoch, y_pred.ravel())\n",
    "    # Compute error\n",
    "    error = y_xor - y_pred\n",
    "    \n",
    "    # Backpropagation\n",
    "    d_out = error * sigmoid_deriv(y_pred)\n",
    "    grad_W2 = a1.T.dot(d_out)\n",
    "    grad_b2 = np.sum(d_out, axis=0)\n",
    "    \n",
    "    error_hidden = d_out.dot(W2.T)\n",
    "    d_hidden = error_hidden * sigmoid_deriv(a1)\n",
    "    grad_W1 = X_xor.T.dot(d_hidden)\n",
    "    grad_b1 = np.sum(d_hidden, axis=0)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    W2 += learning_rate * grad_W2\n",
    "    b2 += learning_rate * grad_b2\n",
    "    W1 += learning_rate * grad_W1\n",
    "    b1 += learning_rate * grad_b1\n",
    "\n",
    "# Evaluate the network on XOR inputs\n",
    "z1 = X_xor.dot(W1) + b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = a1.dot(W2) + b2\n",
    "y_pred = sigmoid(z2)\n",
    "print(\"XOR Predictions after training:\", y_pred.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08da1674-e06c-4ed2-806b-304f0544b371",
   "metadata": {},
   "source": [
    "# A More General Implementation\n",
    "\n",
    "Our custom implementation of neural networks is organized into three main parts:\n",
    "\n",
    "1. **Layers:**  \n",
    "   These are the building blocks of our network. We implement various types of layers such as fully connected (linear) layers and activation functions (e.g., ReLU, Sigmoid, and Identity). Each layer computes a forward pass and stores the necessary information for backpropagation, allowing gradients to be computed and parameters to be updated during training.\n",
    "\n",
    "2. **Cost Functions:**  \n",
    "   Cost (or loss) functions quantify the difference between the network's predictions and the true target values. In our implementation, we provide common loss functions such as Mean Squared Error (MSE) and L1 Loss. These functions play a crucial role in training by providing the signal that drives parameter updates.\n",
    "\n",
    "3. **Optimizers:**  \n",
    "   Optimizers are algorithms that adjust the network's parameters based on the computed gradients. We include several optimizers—a simple Stochastic Gradient Descent (SGD) and more advanced methods like RMSProp and Adam. These optimizers help improve convergence and training stability by adapting the learning process to the geometry of the loss graph.\n",
    "\n",
    "This implementation is inspired by modern deep learning frameworks like PyTorch. It achieves modularity by separating the core components, making it easy to experiment with different architectures, loss functions, and optimization strategies. In the following sections, we detail the implementation of each part and demonstrate how they come together to build and train a neural network.\n",
    "\n",
    "At the core of modern deep learning libraries is an implementation of automatic differentiation. Automatic differentiation is a computational technique that applies the chain rule to compute gradients of complex functions, enabling the efficient and easy to use optimization in deep learning. In our implementation, we mimic this process by defining a backward method for each module (e.g., Linear layers, activation functions, and sequential containers) that computes and propagates local gradients. During the forward pass, each module caches intermediate values needed for differentiation. Then, in the backward pass, these caches are used to compute the derivative of the output with respect to the input, effectively propagating gradients back through the network. This manual implementation of automatic differentiation allows us to update model parameters using gradient-based optimizers such as Adam, thereby minimizing the loss function. Modern frameworks like PyTorch handle automatic differentiation internally and the mechanics are mostly hidden from the user. The point of implementing it ourselves is to show the core principles behind it.\n",
    "\n",
    "## Parameter Object (mimicking `torch.nn.Parameter`)\n",
    "\n",
    "First, we define the `Parameter` class which acts as a container for trainable parameters (such as weights and biases). The `Parameter` class functions as a container for trainable parameters, including weights and biases. Each `Parameter` holds its data as well as a gradient array (initialized to zeros) that will be populated during backpropagation. This design is inspired by PyTorch's `torch.nn.Parameter`, and it simplifies the process of updating parameters during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d5b6d-f856-451d-951d-a80db8a1a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "np.random.seed(0) \n",
    "class Parameter:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = np.zeros_like(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd4c4b3-6240-40a2-9223-56e4f05333be",
   "metadata": {},
   "source": [
    "## Base Module Class (mimicking `torch.nn.Module`)\n",
    "\n",
    "This cell defines the abstract base class `Module`. This class is the foundation for all layers and functions in our neural network. Every module (layer, activation, etc.) must implement the `forward` method (to compute outputs) and the `backward` method (to compute gradients). Additionally, modules can expose their trainable parameters via the `parameters()` method, and `zero_grad()` resets gradients. This structure is similar to PyTorch's `nn.Module` and provides an easy to use interface for gradient based optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaba8e8-8e3c-47b1-80ec-6dabf53bdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parameters(self):\n",
    "        # Return a list of parameters in this module.\n",
    "        return []\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = np.zeros_like(p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed92311d-b6cc-4b23-bc47-1e8b03cc97a2",
   "metadata": {},
   "source": [
    "## Linear Module (Fully Connected Layer)\n",
    "\n",
    "This cell implements a fully connected (or linear) layer via the `Linear` class. Note that in this implementation we use row vectors instead of column vectors. Strictly following the notation above we would therefore write the layer as:  \n",
    "$$ \\mathbf y = \\mathbf W\\mathbf x^\\top + \\mathbf b=\\mathbf x\\mathbf W^\\top + \\mathbf b $$  \n",
    "where $ \\mathbf W $ (weights) and $ \\mathbf b $ (biases) are initialized appropriately—using He initialization in our case—to improve training stability.\n",
    "- The `forward` method computes the layer's output and caches the input for use in backpropagation.  \n",
    "- The `backward` method calculates gradients with respect to the inputs, weights, and biases.  \n",
    "- The `parameters` method returns the list of trainable parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e124cd-4a80-40a3-8881-97f18dce47fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features, init='he'):\n",
    "        if init == 'he':\n",
    "            weight_data = np.random.randn(out_features, in_features) * np.sqrt(2 / in_features)\n",
    "        else:\n",
    "            weight_data = np.random.randn(out_features, in_features)\n",
    "        self.weight = Parameter(weight_data)\n",
    "        self.bias = Parameter(np.zeros((1, out_features)))\n",
    "        self.cache = None  # To store input for backward\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cache = x\n",
    "        return x.dot(self.weight.data.T) + self.bias.data\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        x = self.cache\n",
    "        batch_size = x.shape[0]\n",
    "        self.weight.grad = grad_output.T.dot(x) / batch_size\n",
    "        self.bias.grad = np.sum(grad_output, axis=0, keepdims=True) / batch_size\n",
    "        return grad_output.dot(self.weight.data)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight, self.bias]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3251eb9-357f-4f75-ad92-3f4c4743249a",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Here, we implement several activation functions as subclasses of `Module`.  \n",
    "- **ReLU (Rectified Linear Unit):** Outputs the input if positive, or zero otherwise.  \n",
    "- **Sigmoid:** Maps inputs to the range (0, 1), useful in binary classification tasks.\n",
    "- **Tanh:** Maps inputs to the range (-1, 1).\n",
    "- **Identity:** Returns the input unchanged, which is useful for the output layer in regression tasks.\n",
    "\n",
    "Each activation function defines its own `forward` pass (computing the activated output) and `backward` pass (computing the derivative necessary for backpropagation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c143738e-e046-466f-8500-6c0a3fca6bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def forward(self, x):\n",
    "        self.cache = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        x = self.cache\n",
    "        return grad_output * (x > 0).astype(x.dtype)\n",
    "\n",
    "class Sigmoid(Module):\n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-x))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.out * (1 - self.out)\n",
    "\n",
    "class Tanh(Module):\n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (1 - self.out**2)\n",
    "        \n",
    "class Identity(Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084afcb0-4c10-474f-a7c4-19829e3a6dc4",
   "metadata": {},
   "source": [
    "## Sequential Module\n",
    "\n",
    "The `Sequential` class serves as a container to chain together multiple modules (layers/activations) in a feed-forward manner.  \n",
    "- The `forward` method sequentially applies each module to the input.  \n",
    "- The `backward` method propagates gradients backward through the modules in reverse order.  \n",
    "- The `parameters` method aggregates all trainable parameters from the contained modules.  \n",
    "This design is derived from `torch.nn.Sequential`. `Sequential` greatly simplifies model construction, as it concatenates the modules on its own and we just have to pay attention that the modules we initialize it with, have the correct dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77df80f-88d7-4374-a2ad-43c874e812d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, *modules):\n",
    "        self.modules = modules\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.modules:\n",
    "            x = module.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        for module in reversed(self.modules):\n",
    "            grad_output = module.backward(grad_output)\n",
    "        return grad_output\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for module in self.modules:\n",
    "            params.extend(module.parameters())\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = np.zeros_like(p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa016d3-c4c7-474e-82b8-4a2a7df0c9fb",
   "metadata": {},
   "source": [
    "## Loss Functions for Regression Tasks\n",
    "\n",
    "We define loss functions that quantify the error between the network's predictions and the targets.  \n",
    "- **MSELoss (Mean Squared Error):** Measures the average squared difference, which is differentiable and widely used for regression.  \n",
    "- **L1Loss:** Measures the average absolute difference and can be more robust to outliers.  \n",
    "\n",
    "Both loss classes provide a `forward` method to compute the loss value and a `backward` method to calculate the gradient of the loss with respect to the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94735a7e-17eb-40c6-838e-3a0164eb726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def forward(self, prediction, target):\n",
    "        self.prediction = prediction\n",
    "        self.target = target     \n",
    "        return np.mean(0.5 * (prediction - target) ** 2)\n",
    "\n",
    "    def backward(self):\n",
    "        batch_size = self.target.shape[0]\n",
    "        return (self.prediction - self.target) / batch_size\n",
    "\n",
    "class L1Loss:\n",
    "    def forward(self, prediction, target):\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        return np.mean(np.abs(prediction - target))\n",
    "\n",
    "    def backward(self):\n",
    "        batch_size = self.target.shape[0]\n",
    "        return np.sign(self.prediction - self.target) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9574b17-c5db-41be-8369-1bc1f61d8b17",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "In this cell, we implement several optimization algorithms that update the network's parameters based on computed gradients. In the end, they are all based on gradient descent. `RMSProp` and `Adam` implement different acceleration methods.  \n",
    "- **SGD (Stochastic Gradient Descent):** A basic optimizer that updates parameters in the direction of the negative gradient.  \n",
    "- **RMSProp:** Adapts the learning rate for each parameter by maintaining a running average of squared gradients ([Tieleman & Hinton, 2012](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)).\n",
    "- **Adam:** Combines momentum and adaptive learning rates, along with bias correction, to provide robust updates ([Kingma & Ba, 2015](https://arxiv.org/pdf/1412.6980v8.pdf)).\n",
    "\n",
    "These implementations have similar interfaces to the optimizers found in `torch.optim`, where each optimizer has a `step` method to update parameters and a `zero_grad` method to reset gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df0c66f-b133-4e2a-9097-39bc04f81325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, parameters, lr=0.01):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for p in self.parameters:\n",
    "            p.data -= self.lr * p.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "class RMSProp:\n",
    "    def __init__(self, parameters, lr=0.01, beta=0.9, epsilon=1e-8):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.s = {p: np.zeros_like(p.data) for p in self.parameters}\n",
    "        self.step_count = 0\n",
    "\n",
    "    def step(self, lr=None):\n",
    "        self.lr = lr or self.lr\n",
    "        self.step_count += 1\n",
    "        for p in self.parameters:\n",
    "            self.s[p] = self.beta * self.s[p] + (1 - self.beta) * (p.grad ** 2)\n",
    "            correction = 1 - self.beta ** self.step_count\n",
    "            p.data -= self.lr * p.grad / (np.sqrt(self.s[p] / correction) + self.epsilon)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, parameters, lr=0.001, weight_decay=0.0, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.v = {p: np.zeros_like(p.data) for p in self.parameters}\n",
    "        self.s = {p: np.zeros_like(p.data) for p in self.parameters}\n",
    "        self.step_count = 0\n",
    "\n",
    "    def step(self, lr=None):\n",
    "        self.lr = lr or self.lr\n",
    "        self.step_count += 1\n",
    "        for p in self.parameters:\n",
    "            p.grad += self.weight_decay * p.data\n",
    "            self.v[p] = self.beta1 * self.v[p] + (1 - self.beta1) * p.grad\n",
    "            self.s[p] = self.beta2 * self.s[p] + (1 - self.beta2) * (p.grad ** 2)\n",
    "            v_corr = self.v[p] / (1 - self.beta1 ** self.step_count)\n",
    "            s_corr = self.s[p] / (1 - self.beta2 ** self.step_count)\n",
    "            p.data -= self.lr * v_corr / (np.sqrt(s_corr) + self.epsilon)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad = np.zeros_like(p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c79d42-68ae-4b09-9be1-f4efaa458f38",
   "metadata": {},
   "source": [
    "## Regression Task: Fitting a Sine Function\n",
    "\n",
    "In this final section, we test our implementation by training a multilayer perceptron (MLP) to approximate the sine function. The training data consists of 5000 samples uniformly distributed between $0$ and $2\\pi$, with target values computed as the sine of the input.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b610f1f-3de8-43a6-8ec4-9b302ea4d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.linspace(0, 2*np.pi, 5000).reshape(-1, 1)\n",
    "y_train = np.sin(x_train)\n",
    "x_val = (x_train[1]- x_train[0])/2 + np.linspace(0, 2*np.pi, 5000).reshape(-1, 1)\n",
    "y_val = np.sin(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c64d1ae-87f2-4d95-96ae-e342a70bb185",
   "metadata": {},
   "source": [
    "We construct our model using the `Sequential` container, which stacks multiple `Linear` layers with `ReLU` activations, and ends with an `Identity` layer—appropriate for regression. To enable check-pointing we define two helper functions `get_state_dict` and `set_state_dict` which allow us to save and restore model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd8af9-68a0-4638-8d13-342758427329",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    Linear(1, 16),\n",
    "    ReLU(),\n",
    "    Linear(16, 16),\n",
    "    ReLU(),\n",
    "    Linear(16, 1)\n",
    ")\n",
    "\n",
    "def get_state_dict(model):\n",
    "    return [copy.deepcopy(p.data) for p in model.parameters()]\n",
    "\n",
    "def set_state_dict(model, state_dict):\n",
    "    for p, data in zip(model.parameters(), state_dict):\n",
    "        p.data[:] = data\n",
    "\n",
    "best_models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2de831-cb28-4da3-8f9e-b6f295622ace",
   "metadata": {},
   "source": [
    "We then define the loss function (using `MSELoss`) and the optimizer (using `Adam`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae7ce4d-90c0-4c38-9eeb-09e60bb85cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.01, weight_decay=1.e-6)\n",
    "\n",
    "# Training setting\n",
    "num_epochs = 1000\n",
    "batch_size = 64\n",
    "num_batches = int(np.ceil(x_train.shape[0] / batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e06583-9f91-415b-9422-97ec47661545",
   "metadata": {},
   "source": [
    "The training loop iterates over epochs, shuffling the data each epoch and processing it in mini-batches. For each batch, we perform a forward pass, compute the loss, perform a backward pass to calculate gradients, and update the parameters using the optimizer. The loss is printed at the end of each epoch to monitor training progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c0402-e3e4-4f79-87a4-0b206c801d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    permutation = np.random.permutation(x_train.shape[0])\n",
    "    x_shuffled = x_train[permutation]\n",
    "    y_shuffled = y_train[permutation]\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        x_batch = x_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model.forward(x_batch)\n",
    "        loss = loss_fn.forward(predictions, y_batch)\n",
    "        epoch_loss += loss\n",
    "\n",
    "        # Backward pass\n",
    "        grad_loss = loss_fn.backward()\n",
    "        model.backward(grad_loss)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    val_predictions = model.forward(x_val)\n",
    "    val_loss = loss_fn.forward(val_predictions, y_val)\n",
    "    if len(best_models) < 3:\n",
    "        best_models.append((val_loss, copy.deepcopy(model)))\n",
    "        best_models.sort(key=lambda x: x[0])\n",
    "    else:\n",
    "        if val_loss < best_models[-1][0]:\n",
    "            best_models[-1] = (val_loss, copy.deepcopy(model))\n",
    "            best_models.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972efc3-8d4d-4a18-b3c2-096c9de05678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_val, y_val, label=\"Reference (sin)\", color=\"black\", linewidth=2)\n",
    "\n",
    "for i, (val_loss, model) in enumerate(best_models, 1):\n",
    "    predictions = model.forward(x_val)\n",
    "    plt.plot(x_val, predictions, linestyle=\"--\", \n",
    "             label=f\"Best Model {i} (Val Loss: {val_loss:.4f})\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Validation Data: Best 3 Models vs Reference\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435307b-a2f2-48de-9260-5d4eb4eaa036",
   "metadata": {},
   "source": [
    "# Extending the Implementation with a ResNet\n",
    "\n",
    "Residual Networks (ResNets) are a popular architecture for deep learning because their skip (or residual) connections help alleviate the vanishing gradient problem in very deep networks ([He et al., 2015](https://arxiv.org/pdf/1512.03385.pdf)). In a ResNet, the output of a block of layers is added to the block’s input, allowing the network to learn modifications (or residuals) rather than the full transformation. This design makes it easier to train deep networks.\n",
    "\n",
    "Below, we define a `ResidualBlock` class that:\n",
    "- Wraps a small feed-forward network (two linear layers with an activation in between).\n",
    "- Adds a skip connection (with an optional projection if the input and output dimensions differ).\n",
    "  \n",
    "We then show an example ResNet model built using our `Sequential` container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba46c24-fbeb-431b-966b-470f03f354a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(Module):\n",
    "    def __init__(self, module):\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.module.forward(x)\n",
    "        if out.shape != x.shape:\n",
    "            raise ValueError(f\"Shape mismatch in SkipConnection: input shape {x.shape} vs. output shape {out.shape}\")\n",
    "        return x + out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_wrapped = self.module.backward(grad_output)\n",
    "        return grad_output + grad_wrapped\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.module.parameters()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.module.zero_grad()\n",
    "\n",
    "class ResNetBlock(Module):\n",
    "    def __init__(self, features, hidden_features=None, activation=ReLU()):\n",
    "        hidden_features = hidden_features or features\n",
    "        self.block = Sequential(\n",
    "            Linear(features, hidden_features, init='he'),\n",
    "            activation,\n",
    "            Linear(hidden_features, features, init='he')\n",
    "        )\n",
    "        self.res_block = SkipConnection(self.block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.res_block.forward(x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return self.res_block.backward(grad_output)\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.res_block.parameters()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.res_block.zero_grad()\n",
    "\n",
    "model = Sequential(\n",
    "    Linear(1, 16),      # initial projection to higher-dimensional space\n",
    "    ReLU(),\n",
    "    ResNetBlock(16),     # one residual block\n",
    "    ResNetBlock(16),     # second residual block\n",
    "    ResNetBlock(16),     # third residual block\n",
    "    Linear(16, 1),       # final projection back to output dimension\n",
    ")\n",
    "best_models = []\n",
    "\n",
    "# Use the same loss function and optimizer as before.\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.01, weight_decay=1.e-6)\n",
    "\n",
    "# Training settings\n",
    "num_epochs = 1000\n",
    "batch_size = 64\n",
    "num_batches = int(np.ceil(x_train.shape[0] / batch_size))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    permutation = np.random.permutation(x_train.shape[0])\n",
    "    x_shuffled = x_train[permutation]\n",
    "    y_shuffled = y_train[permutation]\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        x_batch = x_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model.forward(x_batch)\n",
    "        loss = loss_fn.forward(predictions, y_batch)\n",
    "        epoch_loss += loss\n",
    "\n",
    "        # Backward pass\n",
    "        grad_loss = loss_fn.backward()\n",
    "        model.backward(grad_loss)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    val_predictions = model.forward(x_val)\n",
    "    val_loss = loss_fn.forward(val_predictions, y_val)\n",
    "    if len(best_models) < 3:\n",
    "        best_models.append((val_loss, copy.deepcopy(model)))\n",
    "        best_models.sort(key=lambda x: x[0])\n",
    "    else:\n",
    "        if val_loss < best_models[-1][0]:\n",
    "            best_models[-1] = (val_loss, copy.deepcopy(model))\n",
    "            best_models.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b7cbd-f9e1-4cbf-804f-81a14ecc9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_val, y_val, label=\"Reference (sin)\", color=\"black\", linewidth=2)\n",
    "\n",
    "for i, (val_loss, model) in enumerate(best_models, 1):\n",
    "    predictions = model.forward(x_val)\n",
    "    plt.plot(x_val, predictions, linestyle=\"--\", \n",
    "             label=f\"Best Model {i} (Val Loss: {val_loss:.4f})\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Validation Data: Best 3 Models vs Reference\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc83c2-cd33-468b-91ce-7b8fb386ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# URL for the Airfoil Self-Noise dataset (from the UCI repository)\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00291/airfoil_self_noise.dat\"\n",
    "filename = \"airfoil_self_noise.dat\"\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "print(\"Dataset downloaded.\")\n",
    "# The dataset has 1503 rows and 6 columns:\n",
    "# Columns 1-5: features (e.g., frequency, angle of attack, chord length, free-stream velocity, and suction side displacement thickness)\n",
    "# Column 6: target (sound pressure level)\n",
    "data = np.loadtxt(filename)\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1].reshape(-1, 1)\n",
    "n = x.shape[0]\n",
    "indices = np.random.permutation(n)\n",
    "n_train = int(0.5 * n)\n",
    "n_val = int(0.2 * n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "train_idx = indices[:n_train]\n",
    "val_idx = indices[n_train:n_train+n_val]\n",
    "test_idx = indices[n_train+n_val:]\n",
    "\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]\n",
    "x_test, y_test = x[test_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a87f44-d537-4069-b3a7-b2670ec106cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    Linear(5, 8),\n",
    "    ReLU(),\n",
    "    SkipConnection(\n",
    "        Sequential(\n",
    "        Linear(8,16),\n",
    "        ReLU(),\n",
    "        ResNetBlock(16),\n",
    "        ReLU(),\n",
    "        Linear(16,8))\n",
    "    ),\n",
    "    ReLU(),\n",
    "    Linear(8, 1),\n",
    ")\n",
    "\n",
    "best_models = []\n",
    "\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1.e-6)\n",
    "\n",
    "# Training setting\n",
    "num_epochs = 10000\n",
    "batch_size = 16\n",
    "num_batches = int(np.ceil(x_train.shape[0] / batch_size))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    permutation = np.random.permutation(x_train.shape[0])\n",
    "    x_shuffled = x_train[permutation]\n",
    "    y_shuffled = y_train[permutation]\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        x_batch = x_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model.forward(x_batch)\n",
    "        loss = loss_fn.forward(predictions, y_batch)\n",
    "        epoch_loss += loss\n",
    "\n",
    "        # Backward pass\n",
    "        grad_loss = loss_fn.backward()\n",
    "        model.backward(grad_loss)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    val_predictions = model.forward(x_val)\n",
    "    val_loss = loss_fn.forward(val_predictions, y_val)\n",
    "    if len(best_models) < 3:\n",
    "        best_models.append((val_loss, copy.deepcopy(model)))\n",
    "        best_models.sort(key=lambda x: x[0])\n",
    "    else:\n",
    "        if val_loss < best_models[-1][0]:\n",
    "            best_models[-1] = (val_loss, copy.deepcopy(model))\n",
    "            best_models.sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d855c-7227-46ea-8e45-dd4135aad4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sort by y_test values, so the plot is easier to read\n",
    "sort_idx = np.argsort(y_test[:, 0])\n",
    "y_test_sorted = y_test[sort_idx]\n",
    "indices = np.arange(len(y_test_sorted))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(indices, y_test_sorted, label=\"Reference\", color=\"black\", s=20)\n",
    "\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, (val_loss, best_model) in enumerate(best_models):\n",
    "    predictions = best_model.forward(x_test)\n",
    "    pred_sorted = predictions[sort_idx]\n",
    "    plt.scatter(indices, pred_sorted, label=f\"Best Model {i+1} (Val Loss: {val_loss:.4f})\", \n",
    "                color=colors[i], marker='x')\n",
    "\n",
    "plt.xlabel(\"Index (sorted by y)\")\n",
    "plt.ylabel(\"y value\")\n",
    "plt.title(\"Validation Data: Predictions vs Reference (Sorted by y)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9ca6aa-5b17-4bfc-95eb-5ef2e9070c8d",
   "metadata": {},
   "source": [
    "While this implementation may not yet achieve the best possible performance, it provides a solid starting point for further tuning and exploration. For example, the the learning rate could be further reduced as the training progresses. Various strategies exist to refine this adaptation and the implementation of the `Adam` optimizer supports this already. Additionally, the network architecture itself can be modified and improved. It is important to note that for very small datasets, neural networks may not be the optimal choice, and other machine learning algorithms might be more suitable. For larger datasets, there are many useful opportunities to extend this code. However, powerful frameworks such as `PyTorch`, `JAX`, and `TensorFlow` offer more efficient and robust implementations of these concepts with state of the art network architectures. Here, our goal was to understand the optimization problem and the basics of gradient-based optimization with automatic differentiation at a low level. If this tutorial has sparked your interest, exploring one of these established frameworks could be a natural next step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
